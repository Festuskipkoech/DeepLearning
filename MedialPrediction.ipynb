{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1qHxSi68oRtmCqWvnVaCj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Festuskipkoech/DeepLearning/blob/main/MedialPrediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Q74wx2zqA0og"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (Input, Dense,Embedding, Flatten, concatenate, Dropout, BatchNormalization, GlobalAveragePooling2D)\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# random seeds for productivity\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "-vNVQBMMNChg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate synthetic data\n",
        "def generate_synthetic_data(n_samples=1000):\n",
        "  # dummy data\n",
        "  images = np.random.rand(n_samples, 224, 224, 3)\n",
        "\n",
        "  # generate the data in tabular form\n",
        "  tabular_data = pd.DataFrame({\n",
        "      'age':np.random.normal(60, 15, n_samples),\n",
        "      'gender':np.random.choice(['M', 'F'], n_samples),\n",
        "      'smoking_status':np.random.choice(['Never', 'Former', 'Current'], n_samples),\n",
        "      'blood_pressure':np.random.normal(130, 20, n_samples),\n",
        "      'cholestrol':np.random.normal(200, 40, n_samples),\n",
        "      'bmi':np.random.normal(25, 5, n_samples)\n",
        "  })\n",
        "  # synthetic labels\n",
        "  labels= np.random.binomial(1, 0.3, n_samples)\n",
        "  return images, tabular_data, labels\n"
      ],
      "metadata": {
        "id": "5IWoyZ7SOU-v"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# process tabular data\n",
        "def preprocess_tabular_data(df):\n",
        "  df_processed =df.copy()\n",
        "\n",
        "  # initialize dict to store preprocessed objects\n",
        "  label_encoders={}\n",
        "  numerical_scaler = StandardScaler()\n",
        "\n",
        "  # categorical columns\n",
        "  categorical_cols = ['gender', 'smoking_status']\n",
        "  numerical_cols = ['age', 'blood_pressure', 'cholestrol', 'bmi']\n",
        "\n",
        "  # label encode categorical varible\n",
        "  for col in categorical_cols:\n",
        "    label_encoders[col] = LabelEncoder()\n",
        "    df_processed[col] = label_encoders[col].fit_transform(df_processed[col])\n",
        "  df_processed[numerical_cols] = numerical_scaler.fit_transform(df_processed[numerical_cols])\n",
        "  return df_processed, label_encoders, numerical_scaler\n",
        "\n"
      ],
      "metadata": {
        "id": "731hlDQOQ77D"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from re import DEBUG\n",
        "# create multi model modal\n",
        "def create_multilmodal_model(image_shape, num_numerical_vars, categorical_cardinalities):\n",
        "  # image stream\n",
        "  image_input = Input(shape = image_shape)\n",
        "  base_model = ResNet50(weights='imagenet', include_top=False, input_tensor=image_input)\n",
        "\n",
        "  x_img = GlobalAveragePooling2D()(base_model.output)\n",
        "  x_img = Dense(512, activation='relu')(x_img)\n",
        "  x_img =BatchNormalization()(x_img)\n",
        "  x_img =Dense(256, activation='relu')(x_img)\n",
        "\n",
        "  # tabular stream -categorical\n",
        "  categorical_inputs =[]\n",
        "  categorical_embeddings =[]\n",
        "\n",
        "  for cardinality in categorical_cardinalities:\n",
        "   input_cat = Input(shape=(1,))\n",
        "   embedding_size = min(50, cardinality // 2)\n",
        "   embedding = Embedding(cardinality, embedding_size)(input_cat)\n",
        "   embedding = Flatten()(embedding)\n",
        "   categorical_embeddings.append(embedding)\n",
        "\n",
        "  # tabular stream-numerical\n",
        "  numerical_input = Input(shape=(num_numerical_vars,))\n",
        "  x_num = BatchNormalization()(numerical_input)\n",
        "\n",
        "  # combine categorical and numerical\n",
        "  x_tab =concatenate(categorical_embeddings + [x_num])\n",
        "  x_tab = Dense (256, activation='relu')(x_tab)\n",
        "  x_tab = Dense(128, activation='relu')(x_tab)\n",
        "\n",
        "  # merge streams\n",
        "  merged = concatenate([x_img, x_tab])\n",
        "  x = Dense(256, activation='relu')(merged)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=Dense(128, activation='relu')(x)\n",
        "  x= Dropout(0.3)(x)\n",
        "\n",
        "  # output\n",
        "  output = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "  # create model\n",
        "  model =Model(inputs=[image_input] + categorical_inputs + [numerical_input], outputs=output)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "cshAtCFKTg-B"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training and evaluation\n",
        "def train_model(model, train_data, val_data, epochs=10, batch_size=32):\n",
        "  # unpack training data\n",
        "  (X_train_img, X_train_cat_list, X_train_num), y_train =train_data\n",
        "  (X_val_img, X_val_cat_list, X_val_num), y_val = val_data\n",
        "\n",
        "  # compile model\n",
        "  model.compile(\n",
        "      optimizer ='adam',\n",
        "      loss='binary_crossentropy',\n",
        "      metrics = ['accuracy', tf.keras.metrics.AUC()]\n",
        "  )\n",
        "\n",
        "  # create data generators for image augmentation\n",
        "  image_datagen = ImageDataGenerator(\n",
        "      rotation_range = 15,\n",
        "      width_shift_range = 0.1,\n",
        "      height_shift_range = 0.1,\n",
        "      zoom_range = 0.1,\n",
        "      horizontal_flip = True\n",
        "  )\n",
        "\n",
        "  # train model\n",
        "  history = model.fit(\n",
        "      [X_train_img] + X_train_cat_list + [X_train_num],\n",
        "      y_train,\n",
        "      validation_data = ([X_val_img] + X_val_cat_list + [X_val_num], y_val),\n",
        "      epochs=epochs,\n",
        "      batch_size=batch_size\n",
        "  )\n",
        "  return history"
      ],
      "metadata": {
        "id": "nfudZjstY-WT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot training history\n",
        "def plot_training_history(history):\n",
        "  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "  # plot loss\n",
        "  ax1.plot(history.history['loss'], label='Training loss')\n",
        "  ax1.plot(history.history['val_loss'], label='Validation Loss')\n",
        "  ax1.set_title('Model Loss')\n",
        "  ax1.set_xlabel('Epoch')\n",
        "  ax1.set_ylabel('Loss')\n",
        "  ax1.legend()\n",
        "\n",
        "  # plot accuracy\n",
        "  ax2.plot(history.history['accuracy'], label='Training accuracy')\n",
        "  ax2.plot(history.history['val_accuracy'], label='Validatin accuracy')\n",
        "  ax2.set_title('Model Accuracy')\n",
        "  ax2.set_xlabel('Epoch')\n",
        "  ax2.set_ylabel('Accuracy')\n",
        "  ax2.legend()\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        ""
      ],
      "metadata": {
        "id": "T2KlF2SHb_Vh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# execution\n",
        "if __name__ == \"__main__\":\n",
        "  # generate synthetic data\n",
        "  images, tabular_data, labels = generate_synthetic_data(1000)\n",
        "  # preprocess tabular data\n",
        "  processed_tabular, label_encoders, numerical_scaler = preprocess_tabular_data(tabular_data)\n",
        ""
      ],
      "metadata": {
        "id": "MaXJ8ivXeXyV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}